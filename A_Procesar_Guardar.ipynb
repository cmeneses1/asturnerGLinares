{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "limited-cornwall",
   "metadata": {},
   "source": [
    "# Procesar el txt con spacy-stanza y guardar en un fichero binario: A_Procesar_Guardar.ipynb\n",
    "Este es el primer *notebook* para procesar el censo de linajes asturianos de García Linares. Si dispone del censo en txt, escriba la dirección al mismo en la variable `direcciontxt`. De lo contrario, continúe usando el pequeño extracto del censo guardado en la dirección `./Documentos/PadronOCRPrueba.txt`.\n",
    "\n",
    "En este *notebook* de *Python* vamos a realizar las siguientes tareas:\n",
    "- Cargamos un documento txt. El `txt` se encuentra en la dirección `direcciontxt`.\n",
    "- Cargamos los módulos `spacy` y `spacy-stanza`. El primero nos ofrece toda la funcionalidad y facilidad de tratamiento de Spacy. El segundo brinda toda la exactitud en los resultados del procesamiento con Stanza. Recuerde que debe tener instalado las librerías de Stanza, desarrollado por la Universidad de Stanford.\n",
    "- Guardamos el documento procesado como un fichero de datos (documento binario). El binario lo guardaremos en la dirección `direccionbin`. De esta forma no tenemos que procesar el documento cada vez que abrimos *Jupyter Lab*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-professor",
   "metadata": {},
   "source": [
    "## Importamos las librerías a usar\n",
    "Lo haremos siempre al comienzo de cada *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_stanza\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Ejecutamos nuestras funciones creadas para la ocasión. Están en el notebook Z_Funciones.ipynb.\n",
    "%run './Z_Funciones.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-master",
   "metadata": {},
   "source": [
    "## Especificamos las direcciones y configuración del pipeline\n",
    "Esta sección es la que podemos personalizar o modificar a conveniencia según el archivo que queramos procesar, dónde lo queremos guardar, etc. Contiene los siguientes datos:\n",
    "- `direcciontxt`: es la dirección del archivo de txt que vamos a procesar con Spacy-Stanza. Nota: cada línea del txt debe ser una frase completa limitada por un punto.\n",
    "- `direcciontxtprueba`: es la dirección de una pequeña parte archivo de txt que vamos a procesar con Spacy-Stanza. Es un archivo más pequeño que podemos usar para pruebas. Nota: cada línea del txt debe ser una frase completa limitada por un punto.\n",
    "- `direccionbin`: es la dirección donde vamos a guardar el archivo procesado por Spacy-Stanza. Es un archivo binario.\n",
    "- `nlpconfig`: es la variable que determina las características del pipeline de procesamiento de spacy-stanza. Por ejemplo, contiene el idioma, cuántas frases procesamos de una sola vez, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "direcciontxt = './Documentos/PadronOCRPrueba.txt'\n",
    "direccionbin = './Documentos/Padronbin'\n",
    "# direcciontxt = './Privado/PadronOCR.txt'\n",
    "# direccionbin = './Privado/PadronbinTOTAL'\n",
    "nlpconfig = {\n",
    "    'name': 'es', # Language code for the language to build the Pipeline in\n",
    "    'tokenize_batch_size': 32, # Enseguida vamos a cambiarlo por la cantidad máxima de palabras que alberga una frase en nuestro documento.\n",
    "    'ner_batch_size': 32 # Enseguida vamos a cambiarlo por la cantidad máxima de palabras que alberga una frase en nuestro documento.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-winter",
   "metadata": {},
   "source": [
    "## Cargamos el archivo de texto\n",
    "Vamos a convertir el archivo de texto a una variable de Python que Spacy-Stanza pueda procesar. La variable se llamará `lineas` porque es una lista que contiene cada una de las lineas del txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el texto\n",
    "with open(direcciontxt, encoding='utf-8') as f:\n",
    "    \n",
    "    # Creamos la variable 'lineas' como una lista vacía que vamos rellenando.\n",
    "    lineas = []\n",
    "    for linea in f:\n",
    "        lineas.append(linea.strip())\n",
    "\n",
    "# Mostremos las primeras 15 líneas.\n",
    "print(lineas[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-wisdom",
   "metadata": {},
   "source": [
    "Vamos a corregir el número máximo de palabras que el pipeline para resolver el NER. Vamos a hacer que sea igual al número máximo de palabras que tienen las frases de nuestro documento, pero aumentado un 25% para tener en cuenta los tokens de puntuación. Usaremos la función `cuenta_palabras_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos las palabras y corregimos el diccionario nlpconfig\n",
    "numero1 = cuenta_palabras_max(direcciontxt)\n",
    "numero2 = int(numero1 * 1.25)\n",
    "\n",
    "nlpconfig['ner_batch_size'] = numero2\n",
    "nlpconfig['tokenize_batch_size'] = numero2\n",
    "\n",
    "print(\"El número de palabras máximas es\", str(numero1) + \".\", \"\\nAñadiendo un 25%, tomaremos el número\", str(numero2) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-water",
   "metadata": {},
   "source": [
    "## Cargamos el pipeline de procesamiento de spacy-stanza\n",
    "\n",
    "Creamos el pipeline (la función que procesa nuestro archivo de análisis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el pipeline de spacy-stanza para resolver el problema NER.\n",
    "nlp = spacy_stanza.load_pipeline(**nlpconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-reproduction",
   "metadata": {},
   "source": [
    "## Aplicamos el pipeline de spacy-stanza\n",
    "Aplicamos el pipeline a nuestro modelo: \n",
    "- En primer lugar, `nlp.pipe` crea un iterable de frases a ser procesadas una a una.\n",
    "- Al ejecutar la orden `list`, no solo estamos convirtiendo el iterable en una lista, sino que también se aplica, frase a frase, la función `nlp`. \n",
    "- Finalmente, unimos cada frase en un solo documento de SpaCy llamado `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc.from_docs(list(nlp.pipe(lineas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-square",
   "metadata": {},
   "source": [
    "# Serializamos\n",
    "La palabra serializar se refiere a guardar nuestro documento procesado `doc` a un archivo binario que se pueda alojar en un disco duro. Así no tenemos que procesar el documento de texto cada vez que abrimos *Jupyter Lab*. Guardamos el binario en la dirección `direccionbin`. Para ello usaremos la función `serializacion` definida en el archivo *funciones_Spacy.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializamos\n",
    "serializacion(doc, direccionbin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-objective",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
