{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comic-vatican",
   "metadata": {},
   "source": [
    "# Nuevas funciones para el procesamiento con Spacy: funciones_spacy\n",
    "Este *notebook* en *Python* contiene una serie de funciones que sintetizan en una sola orden el código de *SpaCy* necesario para realizar determinadas tareas repetitivas. Por ejemplo, serializar o revertir la serialización de una instancia `Doc()`.\n",
    "\n",
    "## Forma de uso\n",
    "Mi recomendación no es copiar cada función cuando sea necesaria, sino, ejecutar este *notebook* en cada uno de los demás en que usamos el código desarrallado en estas prácticas. Debe hacerse de la siguiente forma:\n",
    "- Usamos el comando mágico `%run`.\n",
    "- Lo hacemos escribiendo en una celda `%run <path of the notebook that has to be run>`.\n",
    "- Por ejemplo, `%run './funciones_Spacy.ipynb'`.\n",
    "- Más información en [este link](https://docs.qubole.com/en/latest/user-guide/notebooks-and-dashboards/notebooks/jupyter-notebooks/running-jupy-notebooks.html)\n",
    "\n",
    "## Lista de funciones que contiene este *notebook*\n",
    "- `cuenta_palabras_max`: esta función toma un archivo txt alojado en el disco y devuelve el número máximo de palabras que contiene una frase del documento.\n",
    "- `revertir_serializacion`: toma un archivo binario con un documento serializado y lo convierte de nuevo en una variable de *Python* como una instancia `Doc()`.\n",
    "- `serializacion`: toma un documento y lo guarda en el disco como un archivo de datos, es decir, un archivo binario.\n",
    "- `verifica_matcher`: esta función recibe un patrón para el `PhraseMatcher` de SpaCy, también variables de ayuda para crear una etiqueta de Span útil para la automatización de la extracción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-ontario",
   "metadata": {},
   "source": [
    "## Contador de palabras máximas: cuenta_palabras_max\n",
    "Esta función toma un archivo txt alojado en el disco y devuelve el número máximo de palabras que contiene una frase del documento.\n",
    "\n",
    "### Sintaxis\n",
    "`numero = cuenta_palabras_max(direcciontxt)`\n",
    "### Valor de los parámetros\n",
    "| Parámetro   | Descripción |\n",
    "|:----------- |:-------------|\n",
    "| direcciontxt| Es la dirección donde está el txt|\n",
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuenta_palabras_max(direcciontxt):\n",
    "    \"\"\"\n",
    "    Esta función toma un archivo txt alojado en el disco y devuelve el número máximo de palabras que contiene una frase del documento.\n",
    "    Parámetros: `direcciontxt`: dirección onde está el txt.\n",
    "    Devuelve: `num`: es el número buscado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicializamos la variable numero\n",
    "    numero = -1\n",
    "    \n",
    "    # Abrimos el documento y recorremos sus líneas\n",
    "    with open(direcciontxt, encoding='utf-8') as f:\n",
    "        \n",
    "        # Recorremos sus líneas\n",
    "        for linea in f:\n",
    "            \n",
    "            # Rompemos cada línea en un espacio con split() y contamos el número de palabras con len\n",
    "            lista = linea.split(\" \")\n",
    "            longitud = len(lista)\n",
    "            \n",
    "            # Comparamos `longitud` con `numero` y guardamos el valor mayor.\n",
    "            if longitud > numero:\n",
    "                numero = longitud\n",
    "    \n",
    "    return numero\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-karen",
   "metadata": {},
   "source": [
    "## Revertir la serialización:  `revertir_serializacion`\n",
    "Esta función toma del disco una instancia un binario que contiene una instancia `Doc()` serializada por *Spacy* y lo devuelve a una instancia `Doc()` en una variable de Python.\n",
    "### Sintaxis\n",
    "`doc = revertir_serializacion(direccionbin, nlp, extensiones)`\n",
    "### Valor de los parámetros\n",
    "| Parámetro   | Descripción |\n",
    "|:----------- |:-------------|\n",
    "| direccionbin| Es la dirección donde está el binario.|\n",
    "| nlp         | Es la función pipeline con la que procesamos el binario. |\n",
    "| extensiones   | Si el binario contiene etiquetas definidas por el usuario, debe darse información es éstas para poder rescatarlas.|\n",
    "### Ejemplo de uso\n",
    "Véase un ejemplo al final del *notebook* `B_Añadir_Etiquetas.ipynb`\n",
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revertir_serializacion(direccionbin, nlp, extensiones=None):\n",
    "    \"\"\"\n",
    "    Esta función toma del disco una instancia un binario que contiene una instancia Doc() serializada por Spacy y lo \n",
    "    devuelve a una instancia Doc() en una variable de Python. Si el binario tiene extensiones elaboradas por el usuario\n",
    "    y no se cargan con set_extension, como se hace aquí, no podrán ser leídas posteriormente.\n",
    "    Parámetros: `direccionbin`: direción donde está el archivo binario,\n",
    "                `nlp`: función de procesamiento de lenguaje natural de SpaCy.\n",
    "                `extensiones`: diccionario que recoge la información de las extensiones elaboradas por el usuario y \n",
    "                    que el archivo binario debe tener registradas.\n",
    "    Resultado: `doc`: instancia de Doc de SpaCy.\n",
    "    \"\"\"\n",
    "    print('Nota: esta función requiere los siguientes módulo o funciones:\\n' ,\n",
    "          'import spacy\\n',\n",
    "          'from spacy.tokens import Doc\\n',\n",
    "          'from spacy.tokens import DocBin\\n',\n",
    "          'Si no los usa se producirá un error.')\n",
    "    \n",
    "    doc_bin = DocBin().from_disk(direccionbin)\n",
    "    doc = list(doc_bin.get_docs(nlp.vocab))[0]\n",
    "    \n",
    "    if extensiones != None:\n",
    "        for nombre, dic in extensiones.items():\n",
    "            if dic['tipo'] == 'span':\n",
    "                Span.set_extension(nombre, default=dic['default'], force=True)\n",
    "    \n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-potential",
   "metadata": {},
   "source": [
    "## Serializacion: `serializacion`\n",
    "Esta función toma una instancia `Doc()` de *SpaCy*, lo serializa y guarda en el disco en la dirección que se le da a la función.\n",
    "### Sintaxis\n",
    "`serializacion( doc, direccionbin)`\n",
    "### Valor de los parámetros\n",
    "| Parámetro | Descripción |\n",
    "|:-----------|:-------------|\n",
    "| doc       | Es una instancia `Doc()` de *SpaCy* |\n",
    "| direccionbin| Es la dirección donde se guardará el binario|\n",
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serializacion(doc, direccionbin):\n",
    "    \"\"\"\n",
    "    Esta función toma una instancia `Doc()` de SpaCy, lo serializa y guarda en el disco en la dirección que se le da a la \n",
    "    función.\n",
    "    Parámetros: `doc` (instancia Doc()) y `direccionbin` (direción para guardar el archivo binario).\n",
    "    \"\"\"\n",
    "    print('Nota: esta función requiere los siguientes módulo o funciones:\\n' ,\n",
    "          'import spacy\\n',\n",
    "          'from spacy.tokens import DocBin\\n',\n",
    "          'Si no los usa se producirá un error.')\n",
    "    \n",
    "    # Creamos un DocBin\n",
    "    doc_bin = DocBin(store_user_data=True)\n",
    "    doc_bin.add(doc)\n",
    "    \n",
    "    # Guardamos en el disco.\n",
    "    doc_bin.to_disk(direccionbin)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-eleven",
   "metadata": {},
   "source": [
    "## Crear etiquetas usando patrones: `verifica_matcher`\n",
    "Esta función recibe un patrón para el `PhraseMatcher` de SpaCy, también variables de ayuda para crear una etiqueta de Span útil para la automatización de la extracción.\n",
    "### Sintaxis\n",
    "`serializacion( doc, direccionbin)`\n",
    "### Valor de los parámetros\n",
    "| Parámetro | Descripción |\n",
    "|:-----------|:-------------|\n",
    "| nlp       | Es el pipeline de *SpaCy* |\n",
    "| doc       | Es una instancia `Doc()` de *SpaCy* |\n",
    "| nombre_extensión| Es el nombre de la etiqueta a crear|\n",
    "| PhraseM   | Si se quiere aplicar el PhraseMatcher, se coloca aquí una lista de frases exactas a identificar|\n",
    "| ayuda     | Esta variable sirve para el etiquetado autómatico. Es un diccionario cuyas claves es el texto a identificar y cuyo valor es la etiqueta a asignar|\n",
    "| patrón    | Si no se usa el PhraseMatcher, debe usarse un patrón de búsqueda de SpaCy|\n",
    "\n",
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifica_matcher(nlp, doc, nombre_extensión, PhraseM=False, ayuda=None, quitarPM=False, patrón=None,):\n",
    "    \"\"\"\n",
    "    Esta función recibe un patrón para el `matcher` de SpaCy y una etiqueta a crear o\n",
    "    rellenar interactivamente sobre los resultados del matcher.\n",
    "    \"\"\"\n",
    "    print('Nota: esta función requiere los siguientes módulo o funciones:\\n' ,\n",
    "          'from spacy.matcher import Matcher, PhraseMatcher\\n',\n",
    "          'from spacy.tokens import Span\\n',\n",
    "          'Si no los usa se producirá un error.')\n",
    "    \n",
    "    if PhraseM == False:\n",
    "        # Creamos y aplicamos el Matcher.\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        matcher.add(\"patrón\", patrón)\n",
    "        matches = matcher(doc)\n",
    "        matches_quitar = None\n",
    "        \n",
    "    elif PhraseM != False and quitarPM != False:\n",
    "        # Creamos y aplicamos el Matcher.\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        matcher.add(\"patrón\", nlp.pipe(quitarPM))\n",
    "        matches_quitar = matcher(doc)\n",
    "        # Creamos y aplicamos el PhraseMatcher.\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        matcher.add(\"patrón\", nlp.pipe(PhraseM))\n",
    "        matches = matcher(doc)\n",
    "        \n",
    "    else:\n",
    "        # Creamos y aplicamos el PhraseMatcher.\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        matcher.add(\"patrón\", nlp.pipe(PhraseM))\n",
    "        matches = matcher(doc)\n",
    "        matches_quitar = None\n",
    "        \n",
    "    for elem in range(0,len(matches)):\n",
    "        matches.append(matches[0][1:])\n",
    "        matches.pop(0)\n",
    "        \n",
    "    if matches_quitar != None:\n",
    "        \n",
    "        for elem in range(0,len(matches_quitar)):\n",
    "            matches_quitar.append(matches_quitar[0][1:])\n",
    "            matches_quitar.pop(0)\n",
    "            \n",
    "        # Quitamos las coincidencias que no nos interesa\n",
    "        lista_quitar = []\n",
    "        for start, end in matches:\n",
    "            \n",
    "            for elem in matches_quitar:\n",
    "                startq, endq = elem\n",
    "                \n",
    "                if startq <= start <= end <= endq:\n",
    "                    lista_quitar.append((start, end))\n",
    "                    \n",
    "        for elem in lista_quitar:\n",
    "            matches.pop(matches.index(elem))\n",
    "            \n",
    "    Span.set_extension(nombre_extensión, default=False, force=True)\n",
    "    dic = {}\n",
    "    # Ahora recorremos `matches` y lo mostramos junto con la frase donde lo encontramos\n",
    "    l = len(matches)\n",
    "    print('Se han encontrado', l, 'concordancias con el patrón.')\n",
    "    i = 0\n",
    "    j = []\n",
    "    while i < l:\n",
    "        start, end = matches[i]\n",
    "        match = doc[start:end]\n",
    "        matchtxt = match.text.upper()\n",
    "        line = match.sent\n",
    "        texto = line.text\n",
    "        i += 1\n",
    "        ayuda_usada = True\n",
    "        \n",
    "        if ayuda != None:\n",
    "            info = ayuda.get(texto.upper())\n",
    "            \n",
    "            if info == None and ('EL ' in texto or 'LA ' in texto or 'LUGAR DE ' in texto):\n",
    "                texto2 = texto.upper().replace('EL ', '').replace('LA ', '').replace('LUGAR DE ', '')\n",
    "                info2 = ayuda.get(texto2)\n",
    "                \n",
    "                if info2 == None:\n",
    "                    ayuda_usada = False\n",
    "                    j.append(i - 1)\n",
    "                    \n",
    "                else:\n",
    "                    line._.set(nombre_extensión, info2)\n",
    "                    \n",
    "            elif info == None and not ('EL ' in texto or 'LA ' in texto or 'LUGAR DE ' in texto):\n",
    "                ayuda_usada = False\n",
    "                j.append(i - 1)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                if ';' in info:\n",
    "                    ayuda_usada = False\n",
    "                    j.append(i - 1)\n",
    "                    \n",
    "                else:\n",
    "                    line._.set(nombre_extensión, info)\n",
    "                    \n",
    "        coincidencia = ayuda.get(matchtxt)\n",
    "        if ayuda_usada == False and coincidencia != None:\n",
    "            print('Concordancia número', str(i-1) + '.', 'id de los tokens: ', [start, end])\n",
    "            print('\\nDígame \"sí\" en caso de querer usar', coincidencia, 'como etiqueta de', match.text + '\\n en la línea \"' + texto + '\".\\n',\n",
    "                  'De lo contrario, escriba la etiqueda deseada.\\n',\n",
    "                  'Escriba \"v\", si quiere volverl al match anterior.\\n',\n",
    "                  'Escriba \"parar\", si quiere pasar al siguiente match.')\n",
    "            etiqueta = input('Escriba aquí: ')\n",
    "            print('\\n')\n",
    "            \n",
    "            if etiqueta.lower() == 'parar':\n",
    "                continue\n",
    "                \n",
    "            elif etiqueta.lower() == 'sí' or etiqueta.lower() == 'si':\n",
    "                line._.set(nombre_extensión, coincidencia)\n",
    "                \n",
    "            elif etiqueta.lower() == 'v':\n",
    "                j.pop(-1)\n",
    "                i = j[-1]\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                line._.set(nombre_extensión, etiqueta)\n",
    "                \n",
    "        elif ayuda_usada == False and coincidencia == None:\n",
    "            print('Concordancia número', str(i-1) + '.', 'id de los tokens: ', [start, end])\n",
    "            print('\\nDígame la etiqueta de', match.text + ' en la línea \"' + texto + '\".\\n',\n",
    "                  'Escriba \"v\", si quiere volverl al match anterior.\\n', \n",
    "                  'Escriba \"parar\", si quiere pasar al siguiente match.')\n",
    "            etiqueta = input('Escriba aquí: ')\n",
    "            print('\\n')\n",
    "            \n",
    "            if etiqueta.lower() == 'parar':\n",
    "                continue\n",
    "                \n",
    "            elif etiqueta.lower() == 'v':\n",
    "                j.pop(-1)\n",
    "                i = j[-1]\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                line._.set(nombre_extensión, etiqueta)\n",
    "                \n",
    "        dic[start] = {'start': start, 'end': end, 'id': [line.sent.start, line.sent.end], 'etiqueta': line._.get(nombre_extensión)}\n",
    "        \n",
    "    return dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-medline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
